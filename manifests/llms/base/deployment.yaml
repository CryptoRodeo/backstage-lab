#==============================
# Ollama
#==============================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backstage-ollama
  namespace: backstage-llms
  labels:
    app: backstage-ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backstage-ollama
  template:
    metadata:
      labels:
        app: backstage-ollama
    spec:
      securityContext:
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch
      initContainers:
        - name: preload-models
          image: ollama/ollama:latest
          resources:
            requests:
              cpu: "2"
              memory: "16Gi"
            limits:
              cpu: "3"
              memory: "24Gi"
          command:
            - /bin/sh
            - -c
            - |
              set -euo pipefail
              echo "[init] starting ollama serve..." ; ollama serve & SERVER_PID=$!
              sleep 5
              echo "[init] Pulling tinyllama:latest..." ; ollama pull tinyllama:latest
              echo "[init] Pulling llama3:8b-instruct..." ; ollama pull llama3:8b-instruct
              echo "[init] Pulling codellama:7b..." ; ollama pull codellama:7b
              echo "[init] stopping ollama (pid=$SERVER_PID)" ; kill "$SERVER_PID" || true
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
      containers:
        - name: ollama
          image: ollama/ollama:latest
          resources:
            requests:
              cpu: "2"
              memory: "16Gi"
            limits:
              cpu: "3"
              memory: "24Gi"
          ports:
            - name: ollama-host
              containerPort: 11434
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
          startupProbe:
            httpGet: { path: /, port: 11434 }
            failureThreshold: 30
            periodSeconds: 2
          readinessProbe:
            httpGet: { path: /, port: 11434 }
            initialDelaySeconds: 5
            periodSeconds: 5
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-data
---
#==============================
# Open Web UI
#==============================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: open-webui
  namespace: backstage-llms
  labels:
    app: open-webui
spec:
  replicas: 1
  selector:
    matchLabels:
      app: open-webui
  template:
    metadata:
      labels:
        app: open-webui
    spec:
      securityContext:
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch
      containers:
        - name: open-webui
          image: ghcr.io/open-webui/open-webui:latest
          ports:
            - name: open-webui-http
              containerPort: 8080
          env:
            - name: OLLAMA_BASE_URL
              value: http://ollama.backstage-llms.svc.cluster.local:11434
            # Optional
            # - name: WEBUI_AUTH
            #   value: true
          volumeMounts:
            - name: openwebui-data
              mountPath: /app/backend/data
          readinessProbe:
            httpGet: { path: /, port: 8080 }
            initialDelaySeconds: 5
            periodSeconds: 5
          # Comment out when GPU available
          # resources:
          #   limits:
          #     nvidia.com/gpu: 1
      volumes:
        - name: openwebui-data
          persistentVolumeClaim:
            claimName: openwebui-data


